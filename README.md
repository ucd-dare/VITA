# 🌊 VITA: Vision-to-Action Flow Matching Policy

Coming soon...

<p align="center">
  <img src="https://img.shields.io/badge/arXiv-2507.13231-b31b1b.svg" alt="arXiv">
  <a href="https://ucd-dare.github.io/VITA/"><img src="https://img.shields.io/badge/Project%20Page-%F0%9F%94%8D-blue" alt="Project Page"></a>
  <a href="https://arxiv.org/pdf/2507.13231"><img src="https://img.shields.io/badge/PDF-%F0%9F%93%84-blue" alt="PDF"></a>
  <img src="https://img.shields.io/badge/License-Apache%202.0-green.svg" alt="License">
</p>

---

**VITA** is a **noise-free and conditioning-free** policy learning framework that learns visuomotor policies by directly flowing from latent images to latent actions.

---

## 🌐 Links

- 🧪 **[Project Page](https://ucd-dare.github.io/VITA/)**
- 📄 **[arXiv Paper](https://arxiv.org/abs/2507.13231)**
- 📑 **[PDF](https://arxiv.org/pdf/2507.13231)**

---

## 📖 Citation

```bibtex
@misc{gao2025vitavisiontoactionflowmatching,
      title={VITA: Vision-to-Action Flow Matching Policy}, 
      author={Dechen Gao and Boqi Zhao and Andrew Lee and Ian Chuang and Hanchu Zhou and Hang Wang and Zhe Zhao and Junshan Zhang and Iman Soltani},
      year={2025},
      eprint={2507.13231},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.13231}, 
}
